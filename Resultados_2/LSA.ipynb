{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oscarandres.pinilla\\AppData\\Local\\Temp\\ipykernel_13244\\266099258.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\oscarandres.pinilla\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\oscarandres.pinilla\\anaconda3\\envs\\tfm\\Lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py:273: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = exp_var / full_var\n",
      "c:\\Users\\oscarandres.pinilla\\anaconda3\\envs\\tfm\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\oscarandres.pinilla\\anaconda3\\envs\\tfm\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\oscarandres.pinilla\\anaconda3\\envs\\tfm\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           BLEU\n",
      "0  0.0007277250\n",
      "1  0.0000000000\n",
      "2  0.0141682980\n",
      "3  0.0042923562\n",
      "4  0.0000091974\n",
      "5  0.0015113701\n",
      "6  0.0000000000\n",
      "7  0.0012691017\n",
      "8  0.0000000000\n",
      "9  0.0125157877\n",
      "10 0.0000000000\n",
      "11 0.0070182728\n",
      "12 0.0000000000\n",
      "13 0.0000001804\n",
      "14 0.0019937322\n",
      "15 0.0000000000\n",
      "16 0.0000000000\n",
      "17 0.0480140617\n",
      "18 0.0000000000\n",
      "19 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Carga del dataset\n",
    "df_ref = pd.read_csv(\"df_ref.csv\")\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "bleu_scores = []\n",
    "\n",
    "def generate_summary(article, num_sentences):\n",
    "    # TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform([article])\n",
    "\n",
    "    lsa_model = TruncatedSVD(n_components=100)\n",
    "    lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "    article_sentences = sent_tokenize(article)\n",
    "    article_scores = lsa_matrix.dot(lsa_matrix.T)\n",
    "    top_sentence_indices = np.argsort(article_scores[0])[-num_sentences:]\n",
    "    summary = ' '.join([article_sentences[i] for i in top_sentence_indices])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "for index, row in df_ref.iterrows():\n",
    "    article = row['article']\n",
    "    original_summary = row['abstract']\n",
    "    \n",
    "    generated_summary = generate_summary(article, num_sentences=3) \n",
    "    \n",
    "    hypothesis_tokens = word_tokenize(generated_summary)\n",
    "    reference_tokens = word_tokenize(original_summary)\n",
    "    \n",
    "    bleu = sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "    \n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "bleu_scores_df = pd.DataFrame({\n",
    "    'BLEU': bleu_scores\n",
    "})\n",
    "\n",
    "pd.options.display.float_format = '{:.10f}'.format\n",
    "\n",
    "print(bleu_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0007277250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0141682980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0042923562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000091974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0015113701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0012691017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0125157877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0070182728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0000001804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0019937322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0480140617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           BLEU\n",
       "0  0.0007277250\n",
       "1  0.0000000000\n",
       "2  0.0141682980\n",
       "3  0.0042923562\n",
       "4  0.0000091974\n",
       "5  0.0015113701\n",
       "6  0.0000000000\n",
       "7  0.0012691017\n",
       "8  0.0000000000\n",
       "9  0.0125157877\n",
       "10 0.0000000000\n",
       "11 0.0070182728\n",
       "12 0.0000000000\n",
       "13 0.0000001804\n",
       "14 0.0019937322\n",
       "15 0.0000000000\n",
       "16 0.0000000000\n",
       "17 0.0480140617\n",
       "18 0.0000000000\n",
       "19 0.0000000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para posteriores cálculos se extrae el dataframe \n",
    "bleu_scores_df.to_csv('BLEU_scores_LSA_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oscarandres.pinilla\\AppData\\Local\\Temp\\ipykernel_8600\\2105809787.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\oscarandres.pinilla\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\oscarandres.pinilla\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\oscarandres.pinilla\\anaconda3\\envs\\tfm\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\oscarandres.pinilla\\anaconda3\\envs\\tfm\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           BLEU\n",
      "0  0.0035135469\n",
      "1  0.0000000000\n",
      "2  0.0004276488\n",
      "3  0.0015172178\n",
      "4  0.0011233799\n",
      "5  0.0011298705\n",
      "6  0.0006139775\n",
      "7  0.0048476089\n",
      "8  0.0027525824\n",
      "9  0.0030455333\n",
      "10 0.0000000000\n",
      "11 0.0043701194\n",
      "12 0.0006217237\n",
      "13 0.0030515849\n",
      "14 0.0048884096\n",
      "15 0.0008728348\n",
      "16 0.0005401870\n",
      "17 0.0013288313\n",
      "18 0.0005875167\n",
      "19 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Carga del dataset\n",
    "df_ref = pd.read_csv(\"df_ref.csv\")\n",
    "\n",
    "bleu_scores = []\n",
    "\n",
    "def generate_summary_lsa(article, num_sentences):\n",
    "    sentences = sent_tokenize(article)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    filtered_sentences = [' '.join([word for word in words if word.lower() not in stop_words]) for words in word_tokens]\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(filtered_sentences)\n",
    "\n",
    "    # Aplicamos LSA\n",
    "    lsa_model = TruncatedSVD(n_components=num_sentences, random_state=42)\n",
    "    lsa_vectors = lsa_model.fit_transform(X)\n",
    "\n",
    "    # Obtenemos los índices de las oraciones más relevantes según LSA\n",
    "    top_sentence_indices = lsa_vectors.argsort()[:, -num_sentences:]\n",
    "\n",
    "    summary = ' '.join([sentences[i] for i in top_sentence_indices.ravel()])\n",
    "\n",
    "    return summary\n",
    "\n",
    "for index, row in df_ref.iterrows():\n",
    "    article = row['article']\n",
    "    original_summary = row['abstract']\n",
    "    \n",
    "    generated_summary = generate_summary_lsa(article, num_sentences=3) \n",
    "    \n",
    "    hypothesis_tokens = word_tokenize(generated_summary)\n",
    "    reference_tokens = word_tokenize(original_summary)\n",
    "    \n",
    "    bleu = sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "    \n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "bleu_scores_df = pd.DataFrame({\n",
    "    'BLEU': bleu_scores\n",
    "})\n",
    "\n",
    "pd.options.display.float_format = '{:.10f}'.format\n",
    "\n",
    "print(bleu_scores_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0035135469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0004276488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0015172178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0011233799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0011298705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0006139775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0048476089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0027525824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0030455333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0043701194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0006217237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0030515849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0048884096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0008728348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0005401870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0013288313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0005875167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           BLEU\n",
       "0  0.0035135469\n",
       "1  0.0000000000\n",
       "2  0.0004276488\n",
       "3  0.0015172178\n",
       "4  0.0011233799\n",
       "5  0.0011298705\n",
       "6  0.0006139775\n",
       "7  0.0048476089\n",
       "8  0.0027525824\n",
       "9  0.0030455333\n",
       "10 0.0000000000\n",
       "11 0.0043701194\n",
       "12 0.0006217237\n",
       "13 0.0030515849\n",
       "14 0.0048884096\n",
       "15 0.0008728348\n",
       "16 0.0005401870\n",
       "17 0.0013288313\n",
       "18 0.0005875167\n",
       "19 0.0000000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para posteriores cálculos se extrae el dataframe \n",
    "bleu_scores_df.to_csv('BLEU_scores_LSA_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
